#
# Copyright 2025 Green-AQI Navigator Team
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

import os
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import joblib
from sqlalchemy import create_engine, text
from dotenv import load_dotenv
from sklearn.preprocessing import MinMaxScaler
from gnn_model import ST_GNN  

# C·∫•u h√¨nh
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
# ƒê·ªß 9 tr·∫°m nh∆∞ y√™u c·∫ßu
HCMC_GRID = [
  { 'id': 'ThuDuc', 'lat': 10.8231, 'lon': 106.7711 },
  { 'id': 'District12', 'lat': 10.8672, 'lon': 106.6415 },
  { 'id': 'HocMon', 'lat': 10.8763, 'lon': 106.5941 },
  { 'id': 'District1', 'lat': 10.7769, 'lon': 106.7009 },
  { 'id': 'BinhTan', 'lat': 10.7656, 'lon': 106.6031 },
  { 'id': 'District2', 'lat': 10.7877, 'lon': 106.7407 },
  { 'id': 'District7', 'lat': 10.734, 'lon': 106.7206 },
  { 'id': 'BinhChanh', 'lat': 10.718, 'lon': 106.6067 },
  { 'id': 'CanGio', 'lat': 10.518, 'lon': 106.8776 },
]
NUM_NODES = len(HCMC_GRID) # = 9
SEQ_LENGTH = 4  
EPOCHS = 100     # TƒÉng epoch l√™n ƒë·ªÉ h·ªçc k·ªπ h∆°n v·ªõi d·ªØ li·ªáu √≠t
LEARNING_RATE = 0.005 # Gi·∫£m learning rate ƒë·ªÉ h·ªôi t·ª• ·ªïn ƒë·ªãnh

def get_db_engine():
    env_path = os.path.join(BASE_DIR, '..', '..', '.env')
    load_dotenv(env_path)
    
    db_host = os.getenv('DB_HOST') or 'postgres-db'
    db_user = os.getenv('DB_USER') or 'admin'
    db_pass = os.getenv('DB_PASS') or 'admin123'
    db_port = os.getenv('DB_PORT') or '5432'
    db_name = os.getenv('DB_NAME') or 'green_aqi_db'
    
    db_url = f"postgresql://{db_user}:{db_pass}@{db_host}:{db_port}/{db_name}"
    return create_engine(db_url)

def load_data_from_db(engine):
    print("üì• ƒêang t·∫£i d·ªØ li·ªáu t·ª´ Database...")
    dfs = []
    for point in HCMC_GRID:
        entity_id = f"urn:ngsi-ld:AirQualityStation:OWM-{point['id']}"
        query = text(f"""
            SELECT time, pm2_5 
            FROM air_quality_observations 
            WHERE entity_id = '{entity_id}'
            ORDER BY time ASC
        """)
        with engine.connect() as conn:
            df = pd.read_sql(query, conn)
            
            # N·∫øu tr·∫°m n√†o ch∆∞a c√≥ d·ªØ li·ªáu th√¨ b·ªè qua (ho·∫∑c x·ª≠ l√Ω fill sau)
            if df.empty:
                print(f"‚ö†Ô∏è C·∫£nh b√°o: Tr·∫°m {point['id']} ch∆∞a c√≥ d·ªØ li·ªáu!")
                return None

            df = df.rename(columns={'pm2_5': point['id']})
            df = df.set_index('time')
            # Fix Warning: D√πng '1h' thay v√¨ '1H'
            df = df.resample('1h').mean().interpolate(method='linear') 
            dfs.append(df)
    
    if not dfs: return None

    # G·ªôp t·∫•t c·∫£ l·∫°i th√†nh 1 b·∫£ng l·ªõn
    dataset = pd.concat(dfs, axis=1).dropna()
    print(f"üìä D·ªØ li·ªáu s·∫°ch ƒë·ªÉ train: {dataset.shape} (Th·ªùi gian x 9 Tr·∫°m)")
    return dataset.values

def create_sequences(data, seq_length):
    # data shape: (Time_Steps, Num_Nodes) -> (381, 9)
    xs, ys = [], []
    for i in range(len(data) - seq_length):
        x = data[i:(i + seq_length)]      # Input: 4 gi·ªù li√™n ti·∫øp
        y = data[i + seq_length]          # Target: Gi·ªù th·ª© 5
        xs.append(x)
        ys.append(y)
    return np.array(xs), np.array(ys)

def train():
    engine = get_db_engine()
    
    # 1. Load Data
    raw_data = load_data_from_db(engine)
    if raw_data is None or len(raw_data) < SEQ_LENGTH + 2:
        print("‚ùå D·ªØ li·ªáu qu√° √≠t ƒë·ªÉ train! H√£y ƒë·ª£i Crawler ch·∫°y th√™m.")
        return

    # 2. Scale Data
    scaler = MinMaxScaler()
    data_scaled = scaler.fit_transform(raw_data)
    
    joblib.dump(scaler, os.path.join(BASE_DIR, 'gnn_scaler.joblib'))
    print("üíæ ƒê√£ l∆∞u Scaler.")

    # 3. T·∫°o Sequence
    # X shape ban ƒë·∫ßu: (Samples, Seq_Len, Nodes) = (N, 4, 9)
    X, y = create_sequences(data_scaled, SEQ_LENGTH)
    
    # üõë FIX QUAN TR·ªåNG: ƒê·ªïi tr·ª•c ƒë·ªÉ kh·ªõp v·ªõi Model
    # Model GNN y√™u c·∫ßu: (Nodes, Seq_Len, Features) = (9, 4, 1) cho m·ªói l·∫ßn ch·∫°y
    # Ta chuy·ªÉn X th√†nh: (Samples, Nodes, Seq_Len) = (N, 9, 4)
    X = np.transpose(X, (0, 2, 1)) 
    
    # Th√™m tr·ª•c Features cu·ªëi c√πng -> (Samples, Nodes, Seq_Len, 1) = (N, 9, 4, 1)
    X = X[..., np.newaxis]         
    
    # Chuy·ªÉn sang Tensor
    X_tensor = torch.tensor(X, dtype=torch.float32)
    y_tensor = torch.tensor(y, dtype=torch.float32)
    
    # Load c·∫•u tr√∫c ƒë·ªì th·ªã
    try:
        edge_index, edge_weight = torch.load(os.path.join(BASE_DIR, 'graph_structure.pt'))
    except:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y graph_structure.pt, vui l√≤ng ch·∫°y script t·∫°o graph tr∆∞·ªõc!")
        return

    # 4. Kh·ªüi t·∫°o Model
    model = ST_GNN(num_nodes=NUM_NODES, input_dim=1, hidden_dim=16, output_dim=1)
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

    print(f"üèãÔ∏è‚Äç‚ôÄÔ∏è B·∫Øt ƒë·∫ßu Train ({EPOCHS} epochs) tr√™n {len(X_tensor)} m·∫´u d·ªØ li·ªáu...")
    model.train()
    
    # üõë V√íNG L·∫∂P TRAIN (S·ª≠a l·ªói 4D Input)
    for epoch in range(EPOCHS):
        total_loss = 0
        
        # Duy·ªát qua t·ª´ng m·∫´u th·ªùi gian (Stochastic Gradient Descent)
        for i in range(len(X_tensor)):
            # L·∫•y 1 m·∫´u ra: X_tensor[i] c√≥ shape (9, 4, 1) -> ƒê√öNG CHU·∫®N 3D
            x_sample = X_tensor[i] 
            y_sample = y_tensor[i] # (9,)
            
            optimizer.zero_grad()
            
            # Forward pass
            output = model(x_sample, edge_index, edge_weight)
            
            # T√≠nh loss: output shape (9, 1) vs y_sample (9,)
            loss = criterion(output.squeeze(), y_sample)
            
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
        
        # In log m·ªói 10 epoch
        if (epoch+1) % 10 == 0:
            avg_loss = total_loss / len(X_tensor)
            print(f"   Epoch {epoch+1}/{EPOCHS}, Avg Loss: {avg_loss:.6f}")

    # 5. L∆∞u Model
    torch.save(model.state_dict(), os.path.join(BASE_DIR, 'gnn_model.pth'))
    print("‚úÖ Train ho√†n t·∫•t! ƒê√£ l∆∞u model m·ªõi v√†o gnn_model.pth")

if __name__ == "__main__":
    train()
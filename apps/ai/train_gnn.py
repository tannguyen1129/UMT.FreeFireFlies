import os
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from sqlalchemy import create_engine, text
from dotenv import load_dotenv
from geopy.distance import geodesic
from sklearn.preprocessing import MinMaxScaler
import joblib
from gnn_model import ST_GNN

# C·∫•u h√¨nh
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
HCMC_GRID = [
  { 'id': 'ThuDuc', 'lat': 10.8231, 'lon': 106.7711 },
  { 'id': 'District12', 'lat': 10.8672, 'lon': 106.6415 },
  { 'id': 'HocMon', 'lat': 10.8763, 'lon': 106.5941 },
  { 'id': 'District1', 'lat': 10.7769, 'lon': 106.7009 },
  { 'id': 'BinhTan', 'lat': 10.7656, 'lon': 106.6031 },
  { 'id': 'District2', 'lat': 10.7877, 'lon': 106.7407 },
  { 'id': 'District7', 'lat': 10.734, 'lon': 106.7206 },
  { 'id': 'BinhChanh', 'lat': 10.718, 'lon': 106.6067 },
  { 'id': 'CanGio', 'lat': 10.518, 'lon': 106.8776 },
]
NUM_NODES = len(HCMC_GRID)
SEQ_LENGTH = 4 
DISTANCE_THRESHOLD_KM = 15.0 # C√°c tr·∫°m c√°ch nhau < 15km s·∫Ω c√≥ c·∫°nh n·ªëi

def get_db_engine():
    env_path = os.path.join(BASE_DIR, '..', '..', '.env')
    load_dotenv(env_path)
    db_url = f"postgresql://{os.getenv('DB_USER')}:{os.getenv('DB_PASS')}@" \
             f"{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/{os.getenv('DB_NAME')}"
    return create_engine(db_url)

# 1. T·∫†O C·∫†NH (EDGE INDEX) D·ª∞A TR√äN KHO·∫¢NG C√ÅCH
def build_graph_edges():
    src_nodes = []
    dst_nodes = []
    weights = []
    
    print("üåê ƒêang x√¢y d·ª±ng ƒë·ªì th·ªã k·∫øt n·ªëi c√°c tr·∫°m...")
    for i in range(NUM_NODES):
        for j in range(NUM_NODES):
            if i == j: continue # Kh√¥ng n·ªëi v·ªõi ch√≠nh n√≥ (ho·∫∑c c√≥ th·ªÉ n·ªëi t√πy m√¥ h√¨nh)
            
            coord_i = (HCMC_GRID[i]['lat'], HCMC_GRID[i]['lon'])
            coord_j = (HCMC_GRID[j]['lat'], HCMC_GRID[j]['lon'])
            dist = geodesic(coord_i, coord_j).km
            
            if dist <= DISTANCE_THRESHOLD_KM:
                src_nodes.append(i)
                dst_nodes.append(j)
                weights.append(1.0 / dist) # Ngh·ªãch ƒë·∫£o kho·∫£ng c√°ch l√†m tr·ªçng s·ªë
    
    edge_index = torch.tensor([src_nodes, dst_nodes], dtype=torch.long)
    edge_weight = torch.tensor(weights, dtype=torch.float)
    print(f"‚úÖ ƒê·ªì th·ªã c√≥ {len(src_nodes)} c·∫°nh k·∫øt n·ªëi.")
    return edge_index, edge_weight

# 2. T·∫¢I V√Ä ƒê·ªíNG B·ªò D·ªÆ LI·ªÜU
def load_synced_data(engine):
    # Ch√∫ng ta c·∫ßn m·ªôt DataFrame l·ªõn ch·ª©a d·ªØ li·ªáu c·ªßa c·∫£ 9 tr·∫°m, index theo th·ªùi gian
    combined_df = pd.DataFrame()
    
    print("üì• ƒêang t·∫£i v√† ƒë·ªìng b·ªô d·ªØ li·ªáu t·ª´ 9 tr·∫°m...")
    for i, node in enumerate(HCMC_GRID):
        query = text(f"SELECT time, pm2_5 FROM air_quality_observations WHERE entity_id = 'urn:ngsi-ld:AirQualityStation:OWM-{node['id']}' ORDER BY time")
        with engine.connect() as conn:
            df = pd.read_sql(query, conn)
            
        df['time'] = pd.to_datetime(df['time'])
        df.set_index('time', inplace=True)
        df = df.resample('15min').mean().interpolate()
        
        # ƒê·ªïi t√™n c·ªôt ƒë·ªÉ merge
        df = df.rename(columns={'pm2_5': f'pm25_{i}'})
        
        if combined_df.empty:
            combined_df = df
        else:
            combined_df = combined_df.join(df, how='inner') # Ch·ªâ l·∫•y m·ªëc th·ªùi gian chung
            
    combined_df.dropna(inplace=True)
    print(f"‚úÖ D·ªØ li·ªáu ƒë·ªìng b·ªô: {len(combined_df)} m·ªëc th·ªùi gian chung.")
    return combined_df

# 3. CHU·∫®N B·ªä DATASET CHO GNN
def create_gnn_dataset(df, seq_len):
    # Output shape: [Num_Samples, Num_Nodes, Seq_Len, Features]
    data_matrix = df.values # [Time, Num_Nodes]
    
    X, y = [], []
    for i in range(len(data_matrix) - seq_len):
        # Input: C·ª≠a s·ªï tr∆∞·ª£t cho T·∫§T C·∫¢ c√°c tr·∫°m
        # Shape: [Num_Nodes, Seq_Len] -> C·∫ßn reshape th√†nh [Num_Nodes, Seq_Len, 1]
        seq = data_matrix[i : i+seq_len].T 
        label = data_matrix[i+seq_len] # Gi√° tr·ªã t∆∞∆°ng lai c·ªßa t·∫•t c·∫£ tr·∫°m
        
        X.append(seq[..., np.newaxis]) # Th√™m dimension feature
        y.append(label)
        
    return torch.tensor(X, dtype=torch.float), torch.tensor(y, dtype=torch.float)

def main():
    engine = get_db_engine()
    
    # 1. X√¢y d·ª±ng Graph
    edge_index, edge_weight = build_graph_edges()
    
    # 2. D·ªØ li·ªáu
    df = load_synced_data(engine)
    if len(df) < 20:
        print("‚ùå Ch∆∞a ƒë·ªß d·ªØ li·ªáu ƒë·ªìng b·ªô ƒë·ªÉ train GNN.")
        return

    # Chu·∫©n h√≥a
    scaler = MinMaxScaler()
    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns, index=df.index)
    
    X, y = create_gnn_dataset(df_scaled, SEQ_LENGTH)
    
    # 3. Model
    model = ST_GNN(num_nodes=NUM_NODES, input_dim=1, hidden_dim=16, output_dim=1)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    criterion = nn.MSELoss()
    
    # 4. Train Loop
    print("üöÄ B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán GNN...")
    model.train()
    for epoch in range(100):
        total_loss = 0
        for i in range(len(X)): # Duy·ªát t·ª´ng snapshot th·ªùi gian
            optimizer.zero_grad()
            
            # Forward: ƒê∆∞a 1 snapshot (9 tr·∫°m, 4 b∆∞·ªõc th·ªùi gian) v√†o
            out = model(X[i], edge_index, edge_weight) 
            
            loss = criterion(out.squeeze(), y[i])
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
            
        if (epoch+1) % 10 == 0:
            print(f"Epoch {epoch+1}, Loss: {total_loss/len(X):.4f}")
            
    # 5. L∆∞u
    torch.save(model.state_dict(), os.path.join(BASE_DIR, 'gnn_model.pth'))
    joblib.dump(scaler, os.path.join(BASE_DIR, 'gnn_scaler.joblib'))
    # L∆∞u edge_index ƒë·ªÉ d√πng l√∫c predict
    torch.save((edge_index, edge_weight), os.path.join(BASE_DIR, 'graph_structure.pt'))
    
    print("‚úÖ Ho√†n t·∫•t hu·∫•n luy·ªán GNN.")

if __name__ == "__main__":
    main()